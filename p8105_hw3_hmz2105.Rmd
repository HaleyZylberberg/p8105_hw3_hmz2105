---
title: "p8105_hw3_hmz2105"
author: Haley Zylberberg
output: github_document
---

```{r setup}
library (tidyverse)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 1

This problem uses data from “The Instacart Online Grocery Shopping Dataset 2017” which is an anonymized dataset with over 3 million online grocery orders from more than 200,000 Instacart users. 

```{r call in data for problem 1}
library(p8105.datasets)
data ("instacart")
```

The instacart data has `r nrow(instacart)` observations and `r ncol(instacart)` variables. The variables in the dataframe are `r names(instacart)`. The department names are `r unique(instacart[["department"]])`. There are `r instacart |> distinct(order_number) |> summarize(order_number = n())` order numbers. The average day of the week that food is ordered is `r instacart |> pull(order_dow) |> mean ()`.

```{r evaluate dataset, results='hide'}
instacart |>
  group_by(product_name) |>
  count(product_name, name = "n_obs") |>
   arrange(desc(n_obs)) 

instacart |>
  group_by(department) |>
  count(department, name = "n_obs") |>
   arrange(desc(n_obs)) 

instacart |>
  group_by(user_id) |>
  count(user_id, name = "n_obs") |>
   arrange(desc(n_obs)) 
```

Bananas are the most ordered product, and was ordered 18726 times. The produce department was the most ordered from, and was ordered from 409,087 times. The user id 149753 ordered the most, with 80 orders.

Next will answer questions regarding aisle information. 

* There are `r instacart |> distinct(aisle_id) |> summarize(aisle_id = n())` aisles in the dataset. 

* The table below shows the 5 aisles that have the most orders (in reverse order), the number of orders, and the aisle names. Aisle 83, which is the fresh vegetable aisle is the most ordered from.

```{r determine most ordered from aisles}
instacart |>
  group_by(aisle) |>
  count(aisle_id, name = "n_obs") |>
   arrange(n_obs) |>
  tail(5) |>
   knitr::kable()
```

*  Plot the number of items ordered in each aisle, limited to aisles with more than 10000 items ordered. 

```{r plot the number items ordered from aisles}
instacart |>
  mutate(aisle_id = as.character(aisle_id)) |>
  count(aisle_id, name = "n_obs") |>
  filter (n_obs > 10000) |>
   ggplot(aes(x = reorder(aisle_id, n_obs), y = n_obs)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Items Ordered From Each Aisle 
       (if greater than 1000 items)",
       x = "Aisle Number",
       y = "Number of Items Ordered")  +
  viridis::scale_fill_viridis(
    name = "aisle_id", 
    discrete = TRUE) +
   theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

Aisles 83 and 24 are the most ordered from aisles, which are the fresh vegetables and fruit aisles. Packaged vegetables and fruits are the next ordered from aisle (123).

* Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r table of 3 most popular items in aisles listed above}
instacart |>
  filter(aisle =="baking ingredients" | aisle == "dog food care"| aisle == "packaged vegetables fruits") |>
  group_by(aisle) |>
  count(product_name, name = "n_obs") |>
  arrange(desc(n_obs)) |>
  slice_head(n = 3) |>
  knitr::kable()

```

Among baking ingredients, light brown sugar is the most ordered item and was ordered 499 times. Among dog food care, Snack Sticks Chicken & Rice Recipe Dog Treats is the most ordered item and was ordered 30 times. Among packaged vegetables and fruits, organic baby spinach is the most ordere item and was ordered 9,785 times.

* Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r table of mean hour of the day per day of the week for 2 food items}
instacart |>
  filter(product_name =="Pink Lady Apples" | product_name == "Coffee Ice Cream") |>
  group_by(product_name, order_dow) |>
  summarize (mean_hour = mean(order_hour_of_day)) |>
   mutate(
 order_dow = case_match(
    order_dow, 0~ 'Sunday', 1 ~ 'Monday', 2~'Tuesday', 3~'Wednesday', 4~'Thursday', 5~'Friday', 6~'Saturday')) |>
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) |>
 knitr::kable(digits = 2, caption = "Mean Hour of the Day per Week Day That Items are Ordered")
```

This table shows that the largest mean hour of the day that Coffee Ice Cream is ordered is the 15.38 hour on Tuesday. The largest mean hour of the day that Pink Lady Apples is ordered is the 14.25 hour on Wednesday. 


## Problem 2

This problem uses data from the Behavioral Risk Factors Surveillance System for Selected Metropolitan Area Risk Trends (SMART) for 2002-2010. 

First we call in the dataset.

```{r call in data for problem 2}
library (p8105.datasets)
brfss_smart2010 
```

The problem will focus on the response to the question, what is "your general health," which is referred to as overall health, and which has responses ordered from fair to excellent. I will re-format the dataset to use appropriate variable names, and limit it to the overall health topic. Next I will organize the responses as a factor taking levels ordered from “Poor” to “Excellent.”

```{r create and filter brfss dataframe}
brfss_smart2010_df =
janitor::clean_names(brfss_smart2010)|>
  filter(topic == "Overall Health") |>
  rename(overall_health = response,
         state = locationabbr,
        county =  locationdesc,
        proportion_responded = data_value) |>
  mutate(overall_health = forcats::fct_relevel(overall_health, c("Poor", "Fair", "Good", "Very good", "Excellent"))) |>
    mutate(county = gsub("County", "", county)) |>
    mutate(county = gsub(".*-", "",county)) |>
  select(year, state, county, overall_health, sample_size, proportion_responded, confidence_limit_low, confidence_limit_high)
```

* Next we will answer which states were observed at 7 or more locations in 2002 and 2010.

  * These are the states with 7 or more observations in 2002:

```{r states with over 7 locations in 2002}
brfss_smart2010_df |>
  filter(year == 2002) |>
  distinct(state, county) |>
  group_by(state) |> 
  summarize(locations = n()) |>
  filter(locations >= 7) |>
  knitr::kable()
```
 There are 6 states with 7 or more locations, with NJ having 8 and PA having 10.

  * These are the states with 7 or more observations in 2010:

```{r states with over 7 locations in 2010}
brfss_smart2010_df |>
  filter(year == 2010) |>
  distinct(state, county) |>
  group_by(state) |> 
  summarize(locations = n()) |>
  filter(locations >= 7) |>
  knitr::kable()
```
 
There are 14 states with 7 or more locations, which is higher than what was recorded in 2002. Florida had the most locations at 41.

* Next, construct a dataset that is limited to Excellent responses, and contains year, state, and a variable that averages the data_value across locations within a state. And then output a “spaghetti” plot of this average value over time within a state.

```{r excellent observations dataset}
excellent_brfss_smart2010_df =
  brfss_smart2010_df |>
  filter(overall_health == "Excellent") |>
  group_by(year, state) |>
  summarize (mean_proportion_responded = mean(proportion_responded)) |>
  select (year, state, mean_proportion_responded) 


excellent_brfss_smart2010_df |>
  ggplot( aes(x = year, y = mean_proportion_responded)) +
   geom_line(aes(group=state)) + geom_smooth() +
  theme(legend.position = "bottom")+
  labs(title = "Mean Proportion Who Responded Excellent to Overall 
  Health Question by State Over Time",
    x = "Year",  
          y = "Mean Proportion Responded Excellent") 
```

This plot shows that from years 2002 through 2010 (which is the duration of the data) the mean proportion of participants who responded "excellent" to the question " How is your overall health" was less than 30 for every state. It seems that most states have an "excellent" response of about 22%. Overall there appears to be a decreasing trend for the "excellent" response over time.  

* Next, make a two-panel plot showing, for the years 2006, and 2010, the distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

```{r plot of proportion responded in NY state by year}
brfss_smart2010_df|>
  filter(state == "NY", year == "2006" | year == "2010") |>
  ggplot(aes(x = county, y=proportion_responded, fill = overall_health)) +
  geom_bar(stat="identity") +
  facet_grid(~year)+
  viridis::scale_fill_viridis(
    name = "county", 
    discrete = TRUE) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(legend.position = "bottom") +
  labs(title = "Proportion Responded to Overall Health Question 
Among Counties in NY For the Years 
2006 and 2010",
    x = "County",  
          y = "Proportion Responded") 

```

This plot shows that among counties in NY, the proportion that responded "Excellent" to the question "How is your overall health" was less than 37.5. Majority of responses in each county for both 2006 and 2010 was in the very good range. It appears that the overall proportion of responses did not change much per county from 2006 to 2010 except that the counties Bronx, Erie, and Monroe were not included in 2006.

## Problem 3

This problem includes data from NHANES regarding participant demographic information and accelerometer data.

* Firt, will import, clean, and merge data.

Iimport demographic data. Skip first 4 rows as they are notes. Remove participants younger than 21 and those with missing data. Recode sex as 1= male and 2= female and education as 1=less than high school, 2= high school, 3=more than high school. Make education a factor variable.

```{r import and tidy demographic data}
Demographic_df = 
  read_csv("data/nhanes_covar.csv", skip = 4) |> 
  janitor::clean_names()|>
  filter(age > 21) |>
  drop_na() |>
  mutate(
 sex = case_match(
    sex, 2~ 'female', 1 ~ 'male'),
 education = case_match(
   education, 1 ~ "less than high school", 2 ~ "high school", 3~ "more than high school")) |>
  mutate(education = forcats::fct_relevel(education, c("less than high school", "high school", "more than high school")))
```

Next, import and tidy accelerometer data. As part of tidying will pivot longer.

```{r import and tidy accelerometer data}
accel_df = 
  read_csv("data/nhanes_accel.csv")|> 
  janitor::clean_names() |>
   pivot_longer(
    min1:min1440,
    names_to = "minute",
    names_prefix = "min",
    values_to = "minute_value"
  ) |>
     mutate(minute = as.numeric(minute))

```

Merge datasets.

```{r merge}
MIMS_df = 
  inner_join(Demographic_df, accel_df, by="seqn")
```

* Next determine the number of men and women in each education level. Represent this data in both table and visual format.

```{r calculate number of men and women per education level }
MIMS_df |> 
  pivot_wider(
    names_from = minute,
    values_from = minute_value
  ) |>
  group_by(sex, education) |> 
  janitor::tabyl(sex, education) |> 
  knitr::kable(digits = 2)

MIMS_df |> 
  pivot_wider(
    names_from = minute,
    values_from = minute_value
  ) |>
ggplot(aes(x = education, fill = sex)) +
  geom_bar(position = "dodge") +
  labs(title = "Count of Men and Women by Education Level",
       x = "Education Level",
       y = "Count") +
  viridis::scale_fill_viridis(
    name = "Education", 
    discrete = TRUE) + 
  theme_minimal() + 
  theme(legend.position = "bottom")
```

Majority of men and women in this dataset have more than a high school degree (n=59 for women and n=54 for men). More men had less than a high school degree than women (34 vs 23).

* Next aggregate accelerometer data for each participant and create a scatter plot of the aggregated accelerometer data for each participant by age, and compare males to females and also group by education.

```{r aggregate accelerometer data for each participant and plot}
MIMS_df |> 
  group_by(seqn) |>
 mutate(total_activity = sum(minute_value)) |>
  ggplot(aes(x = age, y = total_activity, color = sex)) +
    geom_point() +
  geom_smooth(se=FALSE) + 
    facet_grid(~education) +
  theme(legend.position = "bottom")+
  labs(title = "Total Daily Accelerometer Value by Age in Males 
  Compared to Females, Grouped by 
  Education Level",
    x = "Age",  
          y = "Total Daily Accelerometer Data") 
```

This plot shows that for every education level, females that are young (less than age 40) have higher total daily accelerometer data than males. For people with less than a high school degree, it seems that around age 50, males begin to have higher total daily accelerometer data than females. For the other education levels, females seem to have mostly higher values regardless of age.

* Next aggregate accelerometer data by minute to create a variable of mean activity. Then create a scatter plot of mean activity per minute of the day comparing males to females and grouped by education.

```{r aggregate accelerometer data by minute}
MIMS_df|>
  group_by(minute, sex, education) |>
  summarize(mean_activity = mean(minute_value, na.rm = TRUE)) |>
   mutate(education = forcats::fct_relevel(education, c("less than high school", "high school", "more than high school"))) |>
  ggplot(aes(x = minute, y = mean_activity, color = sex)) +
    geom_point() + geom_smooth() + 
    facet_grid(~education) +
  theme(legend.position = "bottom") +
  labs(title = "Mean Accelerometer Value Per Minute in Males 
  Compared to Females, Grouped by Education Level",
    x = "Minute",  
          y = "Mean Accelerometer Value")

```

This graph shows that for every education level, participants have lower mean accelerometer values earlier in the day (time 0-400), then there is a sharp increase with a peak at time 700, and then the values start to decline. For every education level, it seems that males have slightly higher mean values from time 0-400 but then females have higher mean values for the late time points. 